{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fCxkVvBhKfI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NLP FOR TEXT CLASSIFICATION**"
      ],
      "metadata": {
        "id": "QJT0JC5X0G1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"uciml/sms-spam-collection-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "RKvXC8z2NhSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Step 1: Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# ‚úÖ Step 2: Load Dataset\n",
        "url = \"https://raw.githubusercontent.com/justmarkham/pycon-2016-tutorial/master/data/sms.tsv\"\n",
        "df = pd.read_csv(url, sep='\\t', header=None, names=['label', 'message'])\n",
        "\n",
        "# ‚úÖ Step 3: Preprocess Text\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = ''.join([ch for ch in text if ch not in string.punctuation])\n",
        "    tokens = text.split()\n",
        "    stop_words = stopwords.words('english')\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "df['clean_message'] = df['message'].apply(clean_text)\n",
        "\n",
        "# ‚úÖ Step 4: Encode Labels\n",
        "df['label_num'] = df['label'].map({'ham': 0, 'spam': 1})\n",
        "\n",
        "# ‚úÖ Step 5: Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['clean_message'], df['label_num'], test_size=0.2, random_state=42)\n",
        "\n",
        "# ‚úÖ Step 6: TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# ‚úÖ Step 7: Train Model (Logistic Regression)\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_vec, y_train)\n",
        "\n",
        "# ‚úÖ Step 8: Evaluate Model\n",
        "y_pred = model.predict(X_test_vec)\n",
        "\n",
        "print(\"üìä Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "conf_mat = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# ‚úÖ Step 9: Try Predicting Custom SMS Messages\n",
        "sample_sms = [\n",
        "    \"Congratulations! You've won a free ticket to Bahamas. Reply WIN to claim.\",\n",
        "    \"Can we meet tomorrow at 5pm?\",\n",
        "    \"You are selected for a cash prize of $1000. Click the link to claim.\"\n",
        "]\n",
        "\n",
        "sample_clean = [clean_text(msg) for msg in sample_sms]\n",
        "sample_vec = vectorizer.transform(sample_clean)\n",
        "preds = model.predict(sample_vec)\n",
        "\n",
        "for msg, label in zip(sample_sms, preds):\n",
        "    print(f\"\\nüîç Message: {msg}\")\n",
        "    print(f\"‚û° Prediction: {'Spam' if label == 1 else 'Ham'}\")"
      ],
      "metadata": {
        "id": "819u5pGBQiEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ss2kqMEmRWUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ejDU73lsxvol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Step 1: Install & Import Libraries\n",
        "!pip install wordcloud --quiet\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# ‚úÖ Step 2: Load SMS Spam Collection Dataset\n",
        "url = \"https://raw.githubusercontent.com/justmarkham/pycon-2016-tutorial/master/data/sms.tsv\"\n",
        "df = pd.read_csv(url, sep='\\t', header=None, names=['label', 'message'])\n",
        "\n",
        "# ‚úÖ Step 3: Preprocessing Text\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = ''.join(ch for ch in text if ch not in string.punctuation)\n",
        "    tokens = text.split()\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "df['clean_message'] = df['message'].apply(clean_text)\n",
        "df['label_num'] = df['label'].map({'ham': 0, 'spam': 1})\n",
        "\n",
        "# ‚úÖ Step 4: Visualization - Class Distribution\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(data=df, x='label', palette='Set2')\n",
        "plt.title('Distribution of Ham vs Spam')\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "# ‚úÖ Step 5: WordClouds for Spam and Ham\n",
        "spam_words = ' '.join(df[df['label'] == 'spam']['clean_message'])\n",
        "ham_words = ' '.join(df[df['label'] == 'ham']['clean_message'])\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "spam_wc = WordCloud(width=500, height=400, background_color='white', colormap='Reds').generate(spam_words)\n",
        "plt.imshow(spam_wc, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Spam WordCloud')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "ham_wc = WordCloud(width=500, height=400, background_color='white', colormap='Greens').generate(ham_words)\n",
        "plt.imshow(ham_wc, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Ham WordCloud')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ‚úÖ Step 6: Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['clean_message'], df['label_num'], test_size=0.2, random_state=42)\n",
        "\n",
        "# ‚úÖ Step 7: TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# ‚úÖ Step 8: Train a Logistic Regression Classifier\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_vec, y_train)\n",
        "\n",
        "# ‚úÖ Step 9: Predict and Evaluate\n",
        "y_pred = model.predict(X_test_vec)\n",
        "\n",
        "print(\"\\nüìå Text Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Ham', 'Spam']))\n",
        "\n",
        "# ‚úÖ Step 10: Confusion Matrix\n",
        "plt.figure(figsize=(6, 4))\n",
        "conf_mat = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Purples', xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# ‚úÖ Step 11: ROC Curve\n",
        "y_proba = model.predict_proba(X_test_vec)[:, 1]\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % roc_auc, color='darkorange')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# ‚úÖ Step 12: Predict on New Messages\n",
        "print(\"\\nüîç Predicting on Sample Messages:\")\n",
        "\n",
        "sample_sms = [\n",
        "    \"Congratulations! You've won a $1000 gift card. Claim now!\",\n",
        "    \"Are you coming to the meeting today?\",\n",
        "    \"Urgent: Your account is suspended. Click to reactivate.\"\n",
        "]\n",
        "\n",
        "sample_clean = [clean_text(msg) for msg in sample_sms]\n",
        "sample_vec = vectorizer.transform(sample_clean)\n",
        "preds = model.predict(sample_vec)\n",
        "\n",
        "for msg, label in zip(sample_sms, preds):\n",
        "    print(f\"\\nMessage: {msg}\")\n",
        "    print(f\"‚û° Prediction: {'Spam' if label == 1 else 'Ham'}\")"
      ],
      "metadata": {
        "id": "vfMfbyuoyfwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dAsbYRjy4OBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bk-8o6XgU8MR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}